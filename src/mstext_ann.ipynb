{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 6914,
     "status": "ok",
     "timestamp": 1689151440071,
     "user": {
      "displayName": "Micheal Abaho",
      "userId": "00401710427043043754"
     },
     "user_tz": -60
    },
    "id": "NJfGK7XxiikF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2\n",
      "For maximum performance, you can install NMSLIB from sources \n",
      "pip install --no-binary :all: nmslib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import yaml\n",
    "import scispacy\n",
    "from scispacy.abbreviation import AbbreviationDetector\n",
    "from scispacy.linking import EntityLinker\n",
    "from scispacy.umls_utils import UmlsKnowledgeBase\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics import TextAnalyticsClient, HealthcareEntityRelation\n",
    "from pprint import pprint\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VeoPMe6Jxzfp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/18 09:12:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#spark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Mimic-II\").config(\"spark.driver.memory\", \"15g\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 404,
     "status": "ok",
     "timestamp": 1689151446223,
     "user": {
      "displayName": "Micheal Abaho",
      "userId": "00401710427043043754"
     },
     "user_tz": -60
    },
    "id": "BA0xibWo2BcO"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(process)d-%(levelname)s-%(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 411,
     "status": "ok",
     "timestamp": 1689151455525,
     "user": {
      "displayName": "Micheal Abaho",
      "userId": "00401710427043043754"
     },
     "user_tz": -60
    },
    "id": "UW5JxITI1_m9"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#'increase size of notebook'\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "executionInfo": {
     "elapsed": 948,
     "status": "ok",
     "timestamp": 1689151458336,
     "user": {
      "displayName": "Micheal Abaho",
      "userId": "00401710427043043754"
     },
     "user_tz": -60
    },
    "id": "paRm-jaxjOTR",
    "outputId": "0018307b-e28e-4162-c205-9eb08d777d47"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>CLEANED_TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Admission Date: 2151-7-16 Discharge Date: 2151-8-4 Service: ADDENDUM: RADIOLOGIC STUDIES: Radiologic studies also included a chest CT, which confirmed cavitary lesions in the left lung apex consistent with infectious process/tuberculosis. This also moderate-sized left pleural effusion. HEAD CT: Head CT showed no intracranial hemorrhage or mass effect, but old infarction consistent with past medical history. ABDOMINAL CT: Abdominal CT showed lesions of T10 and sacrum most likely secondary to osteoporosis. These can be followed by repeat imaging as an outpatient. First Name8 (NamePattern2) First Name4 (NamePattern1) 1775 Last Name (NamePattern1) , M.D. MD Number(1) 1776 Dictated By:Hospital 1807 MEDQUIST36 D: 2151-8-5 12:11 T: 2151-8-5 12:21 JOB#: Job Number 1808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  \\\n",
       "0      0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           CLEANED_TEXT  \n",
       "0  Admission Date: 2151-7-16 Discharge Date: 2151-8-4 Service: ADDENDUM: RADIOLOGIC STUDIES: Radiologic studies also included a chest CT, which confirmed cavitary lesions in the left lung apex consistent with infectious process/tuberculosis. This also moderate-sized left pleural effusion. HEAD CT: Head CT showed no intracranial hemorrhage or mass effect, but old infarction consistent with past medical history. ABDOMINAL CT: Abdominal CT showed lesions of T10 and sacrum most likely secondary to osteoporosis. These can be followed by repeat imaging as an outpatient. First Name8 (NamePattern2) First Name4 (NamePattern1) 1775 Last Name (NamePattern1) , M.D. MD Number(1) 1776 Dictated By:Hospital 1807 MEDQUIST36 D: 2151-8-5 12:11 T: 2151-8-5 12:21 JOB#: Job Number 1808   "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_read = pd.read_csv('../../mimic-iii/cleaned/notes/NOTESEVENTS_0.csv', low_memory=False)\n",
    "data_read.iloc[:1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 208,
     "status": "ok",
     "timestamp": 1688560125017,
     "user": {
      "displayName": "Micheal Abaho",
      "userId": "00401710427043043754"
     },
     "user_tz": -60
    },
    "id": "wDLgzJpu7X4s",
    "outputId": "1ec2ef31-7277-4da4-d2a4-002be8366b34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12422, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_read.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8462,
     "status": "ok",
     "timestamp": 1688560211958,
     "user": {
      "displayName": "Micheal Abaho",
      "userId": "00401710427043043754"
     },
     "user_tz": -60
    },
    "id": "6npZ8ALFyGx3",
    "outputId": "70b85a1c-4490-440d-d4f7-477eb218bc83"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|index|        CLEANED_TEXT|\n",
      "+-----+--------------------+\n",
      "|    0|Admission Date: 2...|\n",
      "|    1|Admission Date: 2...|\n",
      "|    2|Admission Date: 2...|\n",
      "|    3|Admission Date: 2...|\n",
      "|    4|Admission Date: 2...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#spark\n",
    "# data_read_spark = spark.read.format('csv').option(\"header\", \"true\").load(\"/content/drive/My Drive/Colab Notebooks/mimic-iii-clinical-database-1.4/src/NOTESEVENTS_CLEANED_.csv\")\n",
    "data_read_spark = spark.createDataFrame(data_read)\n",
    "data_read_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 341,
     "status": "ok",
     "timestamp": 1688560219071,
     "user": {
      "displayName": "Micheal Abaho",
      "userId": "00401710427043043754"
     },
     "user_tz": -60
    },
    "id": "U0OccvOS9Dlf",
    "outputId": "c775c801-18ed-4efd-a90a-a85e853f0088"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_read_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vmadmin/mstextanalytics_env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "spacy_model = spacy.load(\"en_core_web_sm\")\n",
    "kb = UmlsKnowledgeBase()\n",
    "st = kb.semantic_type_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 447,
     "status": "ok",
     "timestamp": 1689151958691,
     "user": {
      "displayName": "Micheal Abaho",
      "userId": "00401710427043043754"
     },
     "user_tz": -60
    },
    "id": "96VoTz39mq3m"
   },
   "outputs": [],
   "source": [
    "#specifying microsoft text analytics language service parameters\n",
    "creds = yaml.safe_load(open('creds/text_analytics_credentials.yml', 'r'))\n",
    "endpoint = creds['Subscription']['endpoint']\n",
    "key = creds['Subscription']['key1']\n",
    "text_analytics_client = TextAnalyticsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(key),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1689151960715,
     "user": {
      "displayName": "Micheal Abaho",
      "userId": "00401710427043043754"
     },
     "user_tz": -60
    },
    "id": "KPZlIKlLJ9VJ"
   },
   "outputs": [],
   "source": [
    "def fetch_entitis_span_pos(entity, sentence, extractor):\n",
    "    sentence_tokenized = dict([(i, j) for i, j in enumerate(sentence.split())])\n",
    "    token_ids_cumulative_length = {}\n",
    "    if extractor == 'MSTA4H':\n",
    "        char_offset, entity_len, entity_text = entity.offset, entity.length, entity.text\n",
    "    if extractor == 'MEDCAT':\n",
    "        char_offset, entity_len, entity_text = entity[0], entity[1]-entity[0], entity[2]\n",
    "    curr_offset = 0\n",
    "    span, end_span_found = [], False\n",
    "    if char_offset == 0:\n",
    "        span.append(0)\n",
    "\n",
    "    for i, j in sentence_tokenized.items():\n",
    "        if curr_offset >= char_offset + entity_len:\n",
    "            span.append(i)\n",
    "            end_span_found = True\n",
    "        else:\n",
    "            curr_offset = curr_offset + len(j) + 1\n",
    "            if curr_offset == char_offset:\n",
    "                span.append(i + 1)\n",
    "        if end_span_found == True:\n",
    "            if len(span) != 2:\n",
    "                pass\n",
    "#                 print(\"Failed to detect span position {} {}\".format(entity_text, span))\n",
    "            try:\n",
    "                assert len(span) == 2\n",
    "                break\n",
    "            except Exception as e:\n",
    "                return entity_text, span\n",
    "    return entity_text, span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1689151964530,
     "user": {
      "displayName": "Micheal Abaho",
      "userId": "00401710427043043754"
     },
     "user_tz": -60
    },
    "id": "MKdvDNt_Y6Iq"
   },
   "outputs": [],
   "source": [
    "#create spans of batches that would be used toselect chunks of documents that can be processed by Text analytics without which ratelimitting errors\n",
    "def batching_for_textanalyticsclinet(batch, size):\n",
    "    x, y = 0, batch\n",
    "    batch_list = []\n",
    "    for i in range(0, size, batch):\n",
    "        if i + batch >= size:\n",
    "            batch_list.append((x, x + (size - i)))\n",
    "            break\n",
    "        batch_list.append((x, y))\n",
    "        x = y\n",
    "        y = y + batch\n",
    "    return batch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, end = 0, 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_read = data_read[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443.51146507263184\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "data = data_read[\"CLEANED_TEXT\"].tolist()\n",
    "documents = spacy_model.pipe(data)\n",
    "documents_segmented = []\n",
    "for doc in documents:\n",
    "    doc_sents = doc.sents\n",
    "    docs = []\n",
    "    for i,d in enumerate(doc_sents):\n",
    "        docs.append(\" \".join([i.text for i in d]))\n",
    "    documents_segmented.append(docs)\n",
    "end_time = time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_df = data_read_spark.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_health(x):\n",
    "    poller = text_analytics_client.begin_analyze_healthcare_entities([x])\n",
    "    result = poller.result()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 22922,
     "status": "error",
     "timestamp": 1689151990793,
     "user": {
      "displayName": "Micheal Abaho",
      "userId": "00401710427043043754"
     },
     "user_tz": -60
    },
    "id": "5RdU00ScpLIS",
    "outputId": "b1172515-c069-45a6-f5ed-112ed78f163e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      "**************************************************************************************\n",
      "\n",
      "(0, 6)\n",
      "1 \n",
      "**************************************************************************************\n",
      "\n",
      "(0, 25)\n",
      "(25, 50)\n",
      "(50, 75)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1900-WARNING-There must be an entity whose code didn't get linked to the UMLS KB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 100)\n",
      "(100, 125)\n",
      "(125, 150)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1900-WARNING-There must be an entity whose code didn't get linked to the UMLS KB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 175)\n",
      "(175, 192)\n",
      "2 \n",
      "**************************************************************************************\n",
      "\n",
      "(0, 25)\n",
      "(25, 50)\n",
      "(50, 75)\n",
      "(75, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1900-WARNING-There must be an entity whose code didn't get linked to the UMLS KB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 122)\n",
      "3 \n",
      "**************************************************************************************\n",
      "\n",
      "(0, 25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1900-WARNING-There must be an entity whose code didn't get linked to the UMLS KB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1900-WARNING-There must be an entity whose code didn't get linked to the UMLS KB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 75)\n",
      "(75, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1900-WARNING-There must be an entity whose code didn't get linked to the UMLS KB\n",
      "1900-WARNING-There must be an entity whose code didn't get linked to the UMLS KB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 125)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1900-WARNING-There must be an entity whose code didn't get linked to the UMLS KB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 150)\n",
      "(150, 175)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1900-WARNING-There must be an entity whose code didn't get linked to the UMLS KB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(175, 197)\n",
      "4 \n",
      "**************************************************************************************\n",
      "\n",
      "(0, 25)\n",
      "(25, 50)\n",
      "(50, 75)\n",
      "(75, 100)\n",
      "(100, 125)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1900-WARNING-There must be an entity whose code didn't get linked to the UMLS KB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 150)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1900-WARNING-There must be an entity whose code didn't get linked to the UMLS KB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 175)\n",
      "(175, 193)\n",
      "5 \n",
      "**************************************************************************************\n",
      "\n",
      "(0, 25)\n",
      "(25, 50)\n",
      "(50, 75)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1900-WARNING-There must be an entity whose code didn't get linked to the UMLS KB\n",
      "1900-WARNING-There must be an entity whose code didn't get linked to the UMLS KB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 85)\n",
      "6 \n",
      "**************************************************************************************\n",
      "\n",
      "(0, 25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1900-WARNING-There must be an entity whose code didn't get linked to the UMLS KB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1900-WARNING-There must be an entity whose code didn't get linked to the UMLS KB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 60)\n",
      "7 \n",
      "**************************************************************************************\n",
      "\n",
      "(0, 25)\n",
      "(25, 50)\n",
      "(50, 75)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1900-WARNING-There must be an entity whose code didn't get linked to the UMLS KB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 100)\n",
      "(100, 125)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1900-WARNING-There must be an entity whose code didn't get linked to the UMLS KB\n",
      "1900-WARNING-There must be an entity whose code didn't get linked to the UMLS KB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 132)\n",
      "8 \n",
      "**************************************************************************************\n",
      "\n",
      "(0, 25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1900-WARNING-There must be an entity whose code didn't get linked to the UMLS KB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1900-WARNING-There must be an entity whose code didn't get linked to the UMLS KB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 62)\n",
      "9 \n",
      "**************************************************************************************\n",
      "\n",
      "(0, 25)\n"
     ]
    }
   ],
   "source": [
    "errorsInAnnotation = {}\n",
    "un_linked_codes = {}\n",
    "dataset_ann = []\n",
    "for d,document in enumerate(documents_segmented):\n",
    "    document_batches = batching_for_textanalyticsclinet(25, len(document))\n",
    "    doc_ann = {}\n",
    "    document_batch_entities = []\n",
    "    document_batch_sents = []\n",
    "    document_batch_labels = []\n",
    "    print(d,\"\\n**************************************************************************************\\n\")\n",
    "    sent_id = 0\n",
    "    for k,document_batch in enumerate(document_batches):\n",
    "        print(document_batch)\n",
    "        start, end = document_batch\n",
    "        document_sents = document[start:end]\n",
    "        poller = text_analytics_client.begin_analyze_healthcare_entities(document_sents)\n",
    "        result = poller.result()\n",
    "        # docs = [doc for doc in result if not doc.is_error]\n",
    "        errorSentences = []\n",
    "        sent_entities = []\n",
    "        document_labels = []\n",
    "        for i,doc in enumerate(result):\n",
    "#             print(\"Sent {}: {}\".format(sent_id, document_sents[i]))\n",
    "            if not doc.is_error:\n",
    "                for e_c,entity in enumerate(doc.entities):\n",
    "                    entity_span_pos = fetch_entitis_span_pos(entity, document_sents[i], extractor='MSTA4H')\n",
    "                    entity_annotation = {\"name\": \"{}\".format(entity.text),\n",
    "                                         \"char_pos\": [entity.offset, entity.offset+entity.length],\n",
    "                                         \"Category\":entity.category,\n",
    "                                        \"token_pos\": entity_span_pos[1],\n",
    "                                        \"sent_id\": sent_id,\n",
    "                                        \"score\": entity.confidence_score,\n",
    "                                        \"linked_entities\": []}\n",
    "#                     print(entity)\n",
    "                    if entity.data_sources is not None:\n",
    "                        for data_source in entity.data_sources:\n",
    "                            linked_entity = {}\n",
    "                            if data_source.name.lower() == 'umls':\n",
    "                                try:\n",
    "                                    umls_ent = kb.cui_to_entity[data_source.entity_id]\n",
    "                                    stycodes = [(stycode, st.get_canonical_name(stycode)) for stycode in umls_ent.types]\n",
    "                                    linked_entity[\"kb\"] = data_source.name\n",
    "#                                     print(\"--\",data_source.entity_id, umls_ent.types)\n",
    "                                    for stycode, styname in stycodes:\n",
    "                                        linked_entity[\"id_code\"] = data_source.entity_id\n",
    "                                        linked_entity[\"type\"] = styname\n",
    "                                        linked_entity[\"type_id\"] = stycode\n",
    "#                                         print(\"----\", styname, stycode)\n",
    "                                except Exception as e:\n",
    "                                    linked_entity[\"id_code\"] = data_source.entity_id\n",
    "                                    exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "                                    errorStr = re.sub(r\"<class|>\", \"\", str(exc_type)).strip()\n",
    "                                    if errorStr not in errorsInAnnotation:\n",
    "                                        errorsInAnnotation[errorStr] = [str(e)]\n",
    "                                    else:\n",
    "                                        errorsInAnnotation[errorStr].append(str(e))\n",
    "                                        logging.warning(\"There must be an entity whose code didn't get linked to the UMLS KB\")\n",
    "                            if data_source.name.lower() == 'snomedct_us':\n",
    "                                linked_entity[\"kb\"] = data_source.name\n",
    "                                linked_entity[\"id_code\"] = data_source.entity_id\n",
    "                            if linked_entity:\n",
    "                                entity_annotation[\"linked_entities\"].append(linked_entity)\n",
    "#                                 print(\"===\", linked_entity)\n",
    "                    sent_entities.append(entity_annotation)\n",
    "#                     print(\"Entity {}: {}\".format(e_c, entity))\n",
    "                for relation in doc.entity_relations:\n",
    "                    rel = {}\n",
    "                    try:\n",
    "                        rel[\"r\"] = relation.relation_type\n",
    "                        rt = re.split(r\"Of|Finds\", relation.relation_type)\n",
    "                        for role in relation.roles:\n",
    "                            if role.name == rt[0]:\n",
    "                                rel[\"h\"] = role.name\n",
    "                            elif role.name == rt[1]:\n",
    "                                rel[\"t\"] = role.name\n",
    "                        rel[\"evi\"] = [sent_id]\n",
    "                        document_labels.append(rel)\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "            else:\n",
    "                errorSentences.append((i, doc))\n",
    "                logging.warning(\"Text analytics api didn't process this sentence\")\n",
    "\n",
    "#             print(\"\\n\")\n",
    "            sent_id += 1\n",
    "\n",
    "        if errorSentences:\n",
    "            errorsInAnnotation[d] = errorSentences\n",
    "        document_batch_entities.extend(sent_entities)\n",
    "        document_batch_sents.extend([i.split() for i in document])\n",
    "        document_batch_labels.extend(document_labels)\n",
    "    doc_ann[\"Entities\"] = document_batch_entities\n",
    "    doc_ann[\"Sents\"] = document_batch_sents\n",
    "    doc_ann[\"Labels\"] = document_batch_labels\n",
    "    dataset_ann.append(doc_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2500"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 263,
     "status": "ok",
     "timestamp": 1687712819654,
     "user": {
      "displayName": "Micheal Abaho",
      "userId": "00401710427043043754"
     },
     "user_tz": -60
    },
    "id": "euuBq0989bTs",
    "outputId": "b498bf25-c6f2-4f3c-a8cc-a539a0609a93"
   },
   "outputs": [],
   "source": [
    "def createDir(path):\n",
    "    dest = path\n",
    "    if not os.path.exists(path):\n",
    "        dest = os.makedirs(path)\n",
    "    return dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../anns/microsoft_text_analytics/errors'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dest_dir = '../anns/microsoft_text_analytics/'\n",
    "error_dir = '../anns/microsoft_text_analytics/errors'\n",
    "createDir(dest_dir)\n",
    "createDir(error_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(dest_dir, 'ms_ann_{}_{}.pkl'.format(start, end)), 'bw') as ms:\n",
    "    json.dump(dataset_ann, ms, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    ms.close()\n",
    "          \n",
    "try:\n",
    "    with open(os.path.join(error_dir, 'errors_in_annotation.json'), 'a') as er:\n",
    "        json.dump(errorsInAnnotation, er)\n",
    "        er.close()\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1687713411954,
     "user": {
      "displayName": "Micheal Abaho",
      "userId": "00401710427043043754"
     },
     "user_tz": -60
    },
    "id": "0fZMZexwwiEt",
    "outputId": "3d34b213-09c8-49e3-edba-3fee3941c605"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      "2 <class 'AssertionError'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "l = {}\n",
    "try:\n",
    "  assert 1 == 2\n",
    "except Exception as e:\n",
    "  print(1,e)\n",
    "  exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "  fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "  print(2, exc_type)\n",
    "  if"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 346,
     "status": "ok",
     "timestamp": 1687637797019,
     "user": {
      "displayName": "Micheal Abaho",
      "userId": "00401710427043043754"
     },
     "user_tz": -60
    },
    "id": "dxkG7ynaAidv",
    "outputId": "1df20ab3-b25a-4b5e-de88-d3a3a0645b73"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'KeyError'\": [KeyError('C0743416'),\n",
       "  KeyError('C1999162'),\n",
       "  KeyError('C2237216'),\n",
       "  KeyError('C0471050'),\n",
       "  KeyError('C0470485'),\n",
       "  KeyError('C0517391'),\n",
       "  KeyError('C2183459'),\n",
       "  KeyError('C0470485'),\n",
       "  KeyError('C0742532'),\n",
       "  KeyError('C3508933'),\n",
       "  KeyError('C4029497'),\n",
       "  KeyError('C2041208')]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errorsInAnnotation"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPTZZ9Lcve9JFdfOI91b5W1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mstextanalytics_env",
   "language": "python",
   "name": "mstextanalytics_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
